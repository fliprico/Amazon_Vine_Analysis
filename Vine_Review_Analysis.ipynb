{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNN78cVG2GF+UysKPQPVSqN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install spark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oVtz8ut9iElg","executionInfo":{"status":"ok","timestamp":1674794542016,"user_tz":300,"elapsed":3954,"user":{"displayName":"Derrick Defor","userId":"10692366955653297570"}},"outputId":"a54e0143-5816-4750-a6c4-08d706bf7a50"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting spark\n","  Downloading spark-0.2.1.tar.gz (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.0 KB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 KB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: spark\n","  Building wheel for spark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58761 sha256=27af7679d7342fe3655ef39c5f2b2d1a74264bfa0541a3158e8d8b0dc20fce89\n","  Stored in directory: /root/.cache/pip/wheels/c5/19/ff/9b16f354528bc9698ec3286be7947ebbf1f8391325553961d4\n","Successfully built spark\n","Installing collected packages: spark\n","Successfully installed spark-0.2.1\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"WF0xykxRPb4Q","executionInfo":{"status":"ok","timestamp":1674794544973,"user_tz":300,"elapsed":123,"user":{"displayName":"Derrick Defor","userId":"10692366955653297570"}}},"outputs":[],"source":["import os\n","# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","spark_version = 'spark-3.3.1'\n","#spark_version = 'spark-3.'\n","os.environ['SPARK_VERSION']=spark_version"]},{"cell_type":"code","source":["# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q https://downloads.apache.org/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n","!tar xf $spark-3.3.1-bin-hadoop3.tgz\n","!pip install -q findspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3R8c25k9eLjx","executionInfo":{"status":"ok","timestamp":1674795092667,"user_tz":300,"elapsed":25205,"user":{"displayName":"Derrick Defor","userId":"10692366955653297570"}},"outputId":"44fc248f-0778-447c-c263-d051343a82fe"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Connecting to security.\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [W\r                                                                               \rHit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n","\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [C\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n","Get:5 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n","Hit:6 http://archive.ubuntu.com/ubuntu focal InRelease\n","Hit:7 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n","Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n","Get:12 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\n","Hit:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n","Fetched 336 kB in 2s (158 kB/s)\n","Reading package lists... Done\n","tar: -3.3.1-bin-hadoop3.tgz: Cannot open: No such file or directory\n","tar: Error is not recoverable: exiting now\n"]}]},{"cell_type":"code","source":["# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop3\"\n","     "],"metadata":{"id":"NtzfOgdwghac","executionInfo":{"status":"ok","timestamp":1674794585742,"user_tz":300,"elapsed":126,"user":{"displayName":"Derrick Defor","userId":"10692366955653297570"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Start a SparkSession\n","import findspark\n","findspark.init()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":494},"id":"VdBWEsdsgmCH","executionInfo":{"status":"error","timestamp":1674794587614,"user_tz":300,"elapsed":200,"user":{"displayName":"Derrick Defor","userId":"10692366955653297570"}},"outputId":"fb34ba7f-6973-4dab-d377-fd5f33345eac"},"execution_count":5,"outputs":[{"output_type":"error","ename":"Exception","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-1a20f784af8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Start a SparkSession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfindspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfindspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/findspark.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(spark_home, python_path, edit_rc, edit_profile)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mpy4j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspark_python\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lib\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"py4j-*.zip\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m             raise Exception(\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \"Unable to find py4j in {}, your SPARK_HOME may not be configured correctly\".format(\n\u001b[1;32m    163\u001b[0m                     \u001b[0mspark_python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: Unable to find py4j in /content/spark-3.3.1-bin-hadoop3/python, your SPARK_HOME may not be configured correctly"]}]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"M17-Amazon-Challenge\").config(\"spark.driver.extraClassPath\","],"metadata":{"id":"XuQdP774gwDg","executionInfo":{"status":"aborted","timestamp":1674794289844,"user_tz":300,"elapsed":5,"user":{"displayName":"Derrick Defor","userId":"10692366955653297570"}}},"execution_count":null,"outputs":[]}]}